{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten, Embedding\n",
    "from keras.layers import MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "## Data contains customer review (short sentences) and ratings (1,2,3,4 or 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/DHRUVIL/OneDrive/Stratsntools/Python Directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review Text']=df['Review Text'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "## converting words like 'wouldn't' to 'would not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self,patterns=replacement_patterns):\n",
    "        self.patterns=[(re.compile(regex),repl) for (regex,repl) in patterns]\n",
    "    def replace(self,text):\n",
    "        s=text\n",
    "        for (pattern,repl) in self.patterns:\n",
    "            (s,count)=re.subn(pattern,repl,s)\n",
    "        return s\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "import re\n",
    "replacer= RegexpReplacer()\n",
    "df['Review Text'] = df['Review Text'].apply(lambda x: replacer.replace(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing, Stopwords removal, Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "df['tokenizedx'] = df['Review Text'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "df['tokenizedx']=df['tokenizedx'].apply(lambda x: [y for y in x if y not in stop])\n",
    "\n",
    "\n",
    "df['tokenizedx'] = df['tokenizedx'].apply(lambda x: [y.lower() for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation removal and word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def remove_punct(tokens):\n",
    "    new=[]\n",
    "    for t in tokens:\n",
    "        mt=x.sub(u'',t)\n",
    "        if not mt==u'':\n",
    "            new.append(mt)\n",
    "    return new\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "df['tokenizedx'] = df['tokenizedx'].apply(lambda x: [remove_punct(x)])\n",
    "df['tokenizedx'] = df['tokenizedx'].apply(lambda x: list(chain.from_iterable(x)))\n",
    "\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "df['tokenizedx'] = df['tokenizedx'].apply(lambda x: [segment(y) for y in x])\n",
    "from itertools import chain\n",
    "df['tokenizedx'] = df['tokenizedx'].apply(lambda x: list(chain.from_iterable(x)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text to vectors\n",
    "## We define MAX_NB_WORDS as the maximum number of words we want to keep. The words are selected on the basis of their occurences in the dataset\n",
    "## Max sequence length is the maximum number of tokens we want to see in each review\n",
    "## Embedding dimension is the dimension of the word vectors we want to build\n",
    "## We use the tokenizer to tokenize the sentences (Reviews) and then convert the tokens to a sequence of numbers. The numbers denote the identity assigned to each word (tokens)\n",
    "## To ensure uniform length of the sequences, we pad them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer=Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df['Review Text'])\n",
    "seq=tokenizer.texts_to_sequences(df['Review Text'])\n",
    "seq=pad_sequences(seq,maxlen=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df['Rating'])\n",
    "y=np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.4\n",
    "indices = np.arange(df.shape[0]) # get sequence of row index\n",
    "np.random.shuffle(indices) # shuffle the row indexes\n",
    "data = seq[indices] # shuffle data/product-titles/x-axis\n",
    "category = y[indices] # shuffle labels/category/y-axis\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = category[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = category[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "sentences = []\n",
    "for i in df.index.tolist():\n",
    "    sentences.append(df.loc[i,'tokenizedx'])\n",
    "\n",
    "model=gensim.models.Word2Vec(sentences,min_count=2,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings\n",
    "## We create a numpy zeros array of dimension (words,embedding dimensions) and store vectors from Word2Vec model in rows corresponding to word indices\n",
    "## Word2Vec embeddings are used to initialize the weights of the embedding layer in the subsequent code\n",
    "## +1 in nb_words because we don't have 0 in word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DHRUVIL\\Anaconda_3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in model.wv.vocab.keys():\n",
    "        embedding_matrix[i] = model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer (Does not exactly captures semantics like Word2Vec. Majorly serves the classification purpose)\n",
    "https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## This isn't tuned. But we sure can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(embedding_layer)\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
    "CNN_model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
    "CNN_model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Dense(150,activation='sigmoid'))\n",
    "CNN_model.add(Dense(150,activation='sigmoid'))\n",
    "CNN_model.add(Dense(150,activation='sigmoid'))\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Dense(6,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14092/14092 [==============================] - 11s 815us/step - loss: 0.5123 - acc: 0.7883\n",
      "Epoch 2/5\n",
      "14092/14092 [==============================] - 12s 817us/step - loss: 0.4863 - acc: 0.7985\n",
      "Epoch 3/5\n",
      "14092/14092 [==============================] - 11s 788us/step - loss: 0.4769 - acc: 0.8032\n",
      "Epoch 4/5\n",
      "14092/14092 [==============================] - 11s 783us/step - loss: 0.4497 - acc: 0.8145\n",
      "Epoch 5/5\n",
      "14092/14092 [==============================] - 11s 788us/step - loss: 0.4322 - acc: 0.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1820c378b00>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.fit(x_train, y_train, epochs=5, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 [==============================] - 3s 320us/step\n"
     ]
    }
   ],
   "source": [
    "pred=CNN_model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5004258037044922"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
